# Development Session - [Date Time] - emergency - [incident-description]

## ðŸš¨ EMERGENCY SESSION - CRITICAL INCIDENT

## Incident Overview
- **Start Time**: [Timestamp]
- **Sprint Type**: Emergency Response
- **Severity**: [Critical/High]
- **Incident ID**: [Incident tracking number]
- **Affected Systems**: [Systems/services impacted]
- **User Impact**: [Number of users and impact description]

## Immediate Response (First 5 Minutes)
### Initial Assessment
- [ ] **Service Status**: Check all service health dashboards
- [ ] **Error Rate**: Current error rate and trend
- [ ] **User Reports**: Number and nature of user complaints
- [ ] **Recent Changes**: Last deployment or configuration change

### Emergency Actions
- [ ] **Notify Team**: Alert relevant team members
- [ ] **Incident Channel**: Create incident response channel
- [ ] **Status Page**: Update public status page
- [ ] **Rollback Ready**: Identify rollback options

## Impact Analysis
### Severity Assessment
- **Revenue Impact**: [$ per hour if applicable]
- **User Impact**: [% of users affected]
- **Data Impact**: [Any data loss or corruption]
- **Reputation Impact**: [Public visibility level]

### Affected Components
- **Frontend**: [Specific features/pages affected]
- **Backend**: [APIs/services affected]
- **Database**: [Database impact if any]
- **Third-party**: [External service dependencies]

## Root Cause Investigation
### Timeline of Events
- **[Time]**: Normal operation confirmed
- **[Time]**: First error detected
- **[Time]**: Issue escalated
- **[Time]**: Investigation started

### Initial Hypothesis
1. [Most likely cause]
2. [Second possible cause]
3. [Third possible cause]

## Emergency Debugging Protocol
### Phase 1: Stabilization (10 minutes)
- [ ] **Immediate Mitigation**: Apply temporary fixes
- [ ] **Traffic Management**: Redirect/throttle if needed
- [ ] **Cache Clearing**: Clear problematic caches
- [ ] **Service Restart**: Restart affected services if safe

### Phase 2: Investigation (15 minutes)
- [ ] **Log Analysis**: Check error logs and patterns
- [ ] **Metric Analysis**: Review system metrics
- [ ] **Database Queries**: Check slow/locked queries
- [ ] **External Services**: Verify third-party service status

### Phase 3: Resolution (Variable)
- [ ] **Fix Development**: Develop and test fix
- [ ] **Fix Deployment**: Deploy with monitoring
- [ ] **Verification**: Confirm issue resolved
- [ ] **Monitor**: Watch for recurrence

## Real-time Investigation Log
### [Time] - Initial Response
- Team members online: [Names]
- First actions taken: [Actions]
- Initial findings: [Findings]

### [Time] - Root Cause Identified
- Root cause: [Detailed explanation]
- Evidence: [Logs, metrics, traces]
- Confirmation method: [How confirmed]

### [Time] - Fix Applied
- Fix description: [What was done]
- Deployment method: [How deployed]
- Verification steps: [How verified]

## Communication Log
### Internal Communication
- **[Time]**: [Message to team]
- **[Time]**: [Update to management]
- **[Time]**: [Engineering status update]

### External Communication
- **[Time]**: [Status page update]
- **[Time]**: [Customer communication]
- **[Time]**: [Social media response]

## Technical Details
### Error Details
```
[Error messages, stack traces, relevant logs]
```

### System State
- **CPU Usage**: [% before/during/after]
- **Memory Usage**: [% before/during/after]
- **Database Connections**: [Count and status]
- **Queue Depth**: [Message queue status]

### Configuration at Time of Incident
```yaml
[Relevant configuration that may have contributed]
```

## Resolution Steps
### Immediate Fix
1. [Step 1 of immediate fix]
2. [Step 2 of immediate fix]
3. [Verification of fix]

### Root Cause Fix
1. [Step 1 of permanent fix]
2. [Step 2 of permanent fix]
3. [Testing procedures]
4. [Deployment plan]

## Verification & Monitoring
### Success Criteria
- [ ] Error rate back to normal
- [ ] Response times within SLA
- [ ] No user complaints for 30 minutes
- [ ] All health checks passing

### Post-Fix Monitoring
- [ ] Set up additional alerts
- [ ] Increase monitoring frequency
- [ ] Watch for side effects
- [ ] Track key metrics for 24 hours

## Recovery Actions
### Data Recovery (if needed)
- [ ] Identify affected data
- [ ] Restore from backups
- [ ] Verify data integrity
- [ ] Reconcile any discrepancies

### Service Recovery
- [ ] Clear all caches
- [ ] Reset circuit breakers
- [ ] Verify all integrations
- [ ] Test critical user flows

## Incident Timeline Summary
- **Detection Time**: [When detected]
- **Response Time**: [Time to first response]
- **Resolution Time**: [Time to resolution]
- **Total Downtime**: [Total impact duration]

## Post-Incident Actions
### Immediate (Within 24 hours)
- [ ] Update incident report
- [ ] Notify all stakeholders
- [ ] Schedule post-mortem
- [ ] Document temporary fixes

### Short-term (Within 1 week)
- [ ] Conduct post-mortem
- [ ] Implement permanent fixes
- [ ] Update runbooks
- [ ] Add monitoring/alerts

### Long-term (Within 1 month)
- [ ] Architecture improvements
- [ ] Process improvements
- [ ] Training updates
- [ ] Disaster recovery updates

## Lessons Learned (Quick Capture)
### What Went Wrong
- [Root cause summary]
- [Contributing factors]
- [Process failures]

### What Went Well
- [Effective responses]
- [Good preparations]
- [Team coordination]

### Action Items
- [ ] [High priority improvement]
- [ ] [Medium priority improvement]
- [ ] [Low priority improvement]

## Cost of Incident
- **Revenue Loss**: $[amount]
- **Engineering Hours**: [hours] Ã— [people]
- **Customer Impact**: [Credits/refunds issued]
- **Reputation**: [Social media sentiment]

## Sign-offs
- **Incident Commander**: [Name] - [Time]
- **Technical Lead**: [Name] - [Time]
- **Management**: [Name] - [Time]

---
**Note**: Full post-mortem to be conducted within 48 hours using blameless process.